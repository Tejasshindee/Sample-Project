{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a764ad5-5ee9-47d1-8c5d-a7fd0de87015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: findspark in /home/hadoop/.local/lib/python3.10/site-packages (2.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33e15c16-509e-4ed3-808a-38bdbf56edf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e4ce80-4211-4d4d-9dcd-0612a231c5cc",
   "metadata": {},
   "source": [
    "# Start PySpark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f718055-4966-4256-bbd3-784f4b8efce5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/23 14:21:48 WARN Utils: Your hostname, tejasshinde-Nitro-AN515-55 resolves to a loopback address: 127.0.1.1; using 192.168.1.155 instead (on interface wlp0s20f3)\n",
      "24/02/23 14:21:48 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/02/23 14:21:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"MySparkApp\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adbc16f4-25a9-467c-bb69-9c3e43756f26",
   "metadata": {},
   "source": [
    "# Load the Raw dataset into a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6fd68159-511a-4f5e-8cdc-1a42cf86729c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+-------------------+-------------------+--------------------+-------+------+-------------------+\n",
      "|         event_time|           order_id|         product_id|        category_id|       category_code|  brand| price|            user_id|\n",
      "+-------------------+-------------------+-------------------+-------------------+--------------------+-------+------+-------------------+\n",
      "|2020-04-24 17:20:39|2294359932054536986|1515966223509089906|2268105426648170900|  electronics.tablet|samsung|162.01|1515915625441993984|\n",
      "|2020-04-24 17:20:39|2294359932054536986|1515966223509089906|2268105426648170900|  electronics.tablet|samsung|162.01|1515915625441993984|\n",
      "|2020-04-24 20:07:43|2294444024058086220|2273948319057183658|2268105430162997728|electronics.audio...| huawei| 77.52|1515915625447879434|\n",
      "|2020-04-24 20:07:43|2294444024058086220|2273948319057183658|2268105430162997728|electronics.audio...| huawei| 77.52|1515915625447879434|\n",
      "|2020-04-25 00:46:21|2294584263154074236|2273948316817424439|2268105471367840086|                NULL|karcher|217.57|1515915625443148002|\n",
      "+-------------------+-------------------+-------------------+-------------------+--------------------+-------+------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_path = \"purchase.csv\"\n",
    "purchase_df = spark.read.csv(data_path, header=True, inferSchema=True)\n",
    "\n",
    "purchase_df.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a1766c-b419-4689-a9cc-913a3f5c5469",
   "metadata": {},
   "source": [
    "# Datatypes of each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8482df52-1be7-4e36-a037-9abb4ba257d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('event_time', 'timestamp'),\n",
       " ('order_id', 'bigint'),\n",
       " ('product_id', 'bigint'),\n",
       " ('category_id', 'bigint'),\n",
       " ('category_code', 'string'),\n",
       " ('brand', 'string'),\n",
       " ('price', 'double'),\n",
       " ('user_id', 'bigint')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "purchase_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce921f2-d8b4-475f-a95b-e6215c2eb89b",
   "metadata": {},
   "source": [
    "# Count number of rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28ad9c6c-c816-4db7-b3f5-c80d2f3fe643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2633521\n"
     ]
    }
   ],
   "source": [
    "row_cnt = purchase_df.count()\n",
    "print(row_cnt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33078464-1e43-469a-8573-e4f5c56a5d79",
   "metadata": {},
   "source": [
    "# Drop dulplicate rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30bdbf56-ef7a-4c2f-8e9a-dd9cf297b4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "purchase_df = purchase_df.dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b05762dc-664d-44d7-9ae3-71f9035da893",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/23 14:22:16 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/02/23 14:22:16 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/02/23 14:22:16 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/02/23 14:22:16 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/02/23 14:22:16 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/02/23 14:22:16 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/02/23 14:22:16 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/02/23 14:22:19 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/02/23 14:22:19 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/02/23 14:22:19 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/02/23 14:22:19 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/02/23 14:22:19 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/02/23 14:22:19 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/02/23 14:22:20 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/02/23 14:22:20 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "[Stage 8:===================>                                       (3 + 6) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2632846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "row_cnt = purchase_df.count()\n",
    "print(row_cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1c7f82-61b9-40c0-bac8-717585416409",
   "metadata": {},
   "source": [
    "# Find the minimum and maximum values of event_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9523085e-b065-4ea3-bbae-1d71362f1305",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 15:===========================================>              (6 + 2) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum event_time: 1970-01-01 06:03:40\n",
      "Maximum event_time: 2020-11-21 15:40:30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import min, max\n",
    "\n",
    "min = purchase_df.select(min(\"event_time\")).first()[0]\n",
    "max = purchase_df.select(max(\"event_time\")).first()[0]\n",
    "\n",
    "print(\"Minimum event_time:\", min)\n",
    "print(\"Maximum event_time:\", max)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ad6a2e-8b79-481c-938d-4cc5c8d018bd",
   "metadata": {},
   "source": [
    "# Extract the year from the event_time column and count distinct years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "15677b2e-dfea-466f-bca6-7b8d2c863961",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 18:==============>                                           (2 + 6) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of distinct years: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import year\n",
    "\n",
    "dist_yrs_cnt = purchase_df.select(year(\"event_time\").alias(\"year\")).distinct().count()\n",
    "\n",
    "print(\"Count of distinct years:\", dist_yrs_cnt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e77319d-60ba-424e-8858-811c03cbe09b",
   "metadata": {},
   "source": [
    "# Count records with year 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2101a757-46bf-433a-b803-5786508c006b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/23 14:22:38 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/02/23 14:22:39 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/02/23 14:22:39 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/02/23 14:22:39 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/02/23 14:22:39 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/02/23 14:22:39 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/02/23 14:22:39 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/02/23 14:22:41 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/02/23 14:22:41 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/02/23 14:22:41 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/02/23 14:22:41 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/02/23 14:22:41 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/02/23 14:22:41 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/02/23 14:22:41 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/02/23 14:22:41 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records with year 2020: 2613215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import year\n",
    "\n",
    "rc = purchase_df.filter(year(\"event_time\") == 2020).count()\n",
    "\n",
    "print(\"Number of records with year 2020:\", rc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f640fc49-7f72-49d3-8f59-57bd1e22d532",
   "metadata": {},
   "source": [
    "# Count records with year 1970"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "540e02d6-05e5-462f-a6aa-8e003064483d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 36:=====================>                                    (3 + 5) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records with year 1970: 19631\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import year\n",
    "\n",
    "rc = purchase_df.filter(year(\"event_time\") == 1970).count()\n",
    "\n",
    "print(\"Number of records with year 1970:\", rc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d954149-ea9b-4d30-933f-23b3c9717516",
   "metadata": {},
   "source": [
    "# Keep records of year 2020 only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f545c00d-be03-4d61-9384-0a9129e3f552",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2020 = purchase_df.filter(year(\"event_time\") != 1970)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8b35a11b-38bd-4f7f-b952-bf7767fd6914",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/23 14:23:08 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/02/23 14:23:09 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/02/23 14:23:09 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/02/23 14:23:09 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/02/23 14:23:09 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/02/23 14:23:09 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/02/23 14:23:09 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/02/23 14:23:11 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/02/23 14:23:11 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/02/23 14:23:11 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/02/23 14:23:11 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/02/23 14:23:11 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/02/23 14:23:11 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/02/23 14:23:11 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/02/23 14:23:11 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2613215"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2020.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ff52eb-2cf3-4166-bcdd-ff7b1c0ef629",
   "metadata": {},
   "source": [
    "# Describe the DataFrame with null value counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9ae67997-37a5-4aae-a013-85b1f4b468ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/23 14:23:17 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/02/23 14:23:18 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/02/23 14:23:18 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/02/23 14:23:18 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/02/23 14:23:18 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/02/23 14:23:18 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/02/23 14:23:18 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/02/23 14:23:21 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/02/23 14:23:21 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/02/23 14:23:21 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/02/23 14:23:21 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/02/23 14:23:21 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/02/23 14:23:21 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/02/23 14:23:21 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/02/23 14:23:21 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "[Stage 50:>                                                         (0 + 8) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+----------+-----------+-------------+------+------+-------+\n",
      "|event_time|order_id|product_id|category_id|category_code| brand| price|user_id|\n",
      "+----------+--------+----------+-----------+-------------+------+------+-------+\n",
      "|         0|       0|         0|     427875|       607074|501294|427875|2051027|\n",
      "+----------+--------+----------+-----------+-------------+------+------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "null_cnt = [F.sum(F.col(column).isNull().cast(\"integer\")).alias(column) for column in df_2020.columns]\n",
    "\n",
    "null_cnt_df = df_2020.agg(*null_cnt)\n",
    "\n",
    "null_cnt_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934bd4ef-d1a4-42ce-9848-03bfe94523ec",
   "metadata": {},
   "source": [
    "# Drop records containing null price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0a6396fe-0873-4917-80d7-8e1b9ae51772",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_price = df_2020.na.drop(subset=[\"price\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cdc745b4-b287-480f-be41-620271b2fed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/23 14:24:00 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2185340"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_price.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee25bf8-d8b6-4723-89c1-62249e7d3b34",
   "metadata": {},
   "source": [
    "# Drop records having brand value as null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4a861cc7-b9c8-4224-94cc-61b794774c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_brand = df_price.filter(df_price[\"brand\"].isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9feefb61-e5e8-4597-af59-6334e602ca58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2073432"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_brand.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2c66944e-f1d6-4eb8-a8e9-2868b4a784cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 66:==================================================>       (7 + 1) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+----------+-----------+-------------+-----+-----+-------+\n",
      "|event_time|order_id|product_id|category_id|category_code|brand|price|user_id|\n",
      "+----------+--------+----------+-----------+-------------+-----+-----+-------+\n",
      "|         0|       0|         0|          0|       552013|    0|    0|1538367|\n",
      "+----------+--------+----------+-----------+-------------+-----+-----+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "null_cnt = [F.sum(F.col(column).isNull().cast(\"integer\")).alias(column) for column in df_brand.columns]\n",
    "\n",
    "null_cnt_df = df_brand.agg(*null_cnt)\n",
    "\n",
    "null_cnt_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcfd3a15-74d3-4a07-a401-49a5bc4cff59",
   "metadata": {},
   "source": [
    "# Count the distinct values in the \"user_id\" column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3af3d0dd-53f2-44c8-9735-1b0f2e2a47b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 72:==================================================>       (7 + 1) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of distinct values in the 'user_id' column: 228970\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "dc = df_brand.select(\"user_id\").distinct().count()\n",
    "\n",
    "print(\"Number of distinct values in the 'user_id' column:\", dc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ee64ac-350f-4a01-a008-b3d50258f755",
   "metadata": {},
   "source": [
    "# Check Null values in dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "49572071-9d56-4629-ad67-3ac10c7c7510",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 78:==============>                                           (2 + 6) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+----------+-----------+-------------+-----+-----+-------+\n",
      "|event_time|order_id|product_id|category_id|category_code|brand|price|user_id|\n",
      "+----------+--------+----------+-----------+-------------+-----+-----+-------+\n",
      "|         0|       0|         0|          0|       552013|    0|    0|1538367|\n",
      "+----------+--------+----------+-----------+-------------+-----+-----+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "null_cnt = [F.sum(F.col(column).isNull().cast(\"integer\")).alias(column) for column in df_brand.columns]\n",
    "\n",
    "null_cnt_df = df_brand.agg(*null_cnt)\n",
    "\n",
    "null_cnt_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0140123-214a-440b-8a7f-7b1150bcbdf6",
   "metadata": {},
   "source": [
    "# Assign \"other\" where category_code is null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3061bda2-77c6-40c6-bce7-8120d2786e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cat = df_brand.fillna(\"other\", subset=[\"category_code\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "26b2fd74-681a-4045-a468-ffec344de409",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 84:=============================>                            (4 + 4) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+----------+-----------+-------------+-----+-----+-------+\n",
      "|event_time|order_id|product_id|category_id|category_code|brand|price|user_id|\n",
      "+----------+--------+----------+-----------+-------------+-----+-----+-------+\n",
      "|         0|       0|         0|          0|            0|    0|    0|1538367|\n",
      "+----------+--------+----------+-----------+-------------+-----+-----+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "null_cnt = [F.sum(F.col(column).isNull().cast(\"integer\")).alias(column) for column in df_cat.columns]\n",
    "\n",
    "null_cnt_df = df_cat.agg(*null_cnt)\n",
    "\n",
    "null_cnt_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590e0c70-07e0-4e5f-bbb3-a7322e61cd3f",
   "metadata": {},
   "source": [
    "# Distinct event times "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5c1278fc-edb4-4b0d-95ee-94ee86091594",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 90:=============================>                            (4 + 4) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of distinct values in the 'event_time' column: 1265835\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "dc = df_cat.select(\"event_time\").distinct().count()\n",
    "\n",
    "print(\"Number of distinct values in the 'event_time' column:\", dc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7bae84-e894-42e8-abae-a478bc87e2a7",
   "metadata": {},
   "source": [
    "# Distinct Order IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "65ef3aeb-0cf0-41f5-bd30-f75844f4256e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of distinct values in the 'order_id' column: 1367525\n"
     ]
    }
   ],
   "source": [
    "dc = df_cat.select(\"order_id\").distinct().count()\n",
    "\n",
    "print(\"Number of distinct values in the 'order_id' column:\", dc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803c79fb-aa94-41c1-967d-af76fd60a241",
   "metadata": {},
   "source": [
    "# Check inconsistent records where multiple duplicate order_id have multiple unique event_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0bdf9128-a3fc-4a5f-9ada-b4b496e56336",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 104:>                                                        (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of inconsistent records: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Group by order_id and count the distinct event_time values\n",
    "incon_rec_cnt = df_cat.groupby(\"order_id\").agg(F.countDistinct(\"event_time\").alias(\"unique_event_times\"))\n",
    "\n",
    "# Filter for records where the count of unique event_time values is greater than 1\n",
    "incon_rec_cnt = incon_rec_cnt.filter(incon_rec_cnt[\"unique_event_times\"] > 1)\n",
    "\n",
    "# Count the number of inconsistent records\n",
    "num_incon_rec = incon_rec_cnt.count()\n",
    "\n",
    "print(\"Number of inconsistent records:\", num_incon_rec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4fc52133-78c2-40f5-9f14-c4447786be9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 112:==============>                                          (2 + 6) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+-------------------+-------------------+--------------------+-------+------+-------------------+\n",
      "|         event_time|           order_id|         product_id|        category_id|       category_code|  brand| price|            user_id|\n",
      "+-------------------+-------------------+-------------------+-------------------+--------------------+-------+------+-------------------+\n",
      "|2020-04-29 20:11:49|2298069964415828136|1515966223509122874|2268105407933187062|computers.periphe...|     hp|152.52|1515915625443027224|\n",
      "|2020-04-29 23:42:11|2298175846491357353|1515966223509122666|2268105430162997728|electronics.audio...|samsung|  8.08|1515915625445938216|\n",
      "|2020-04-30 18:01:51|2298729326712980173|1515966223509089265|2360741866917331945|appliances.enviro...|   beko|231.46|1515915625446617606|\n",
      "|2020-04-30 19:27:36|2298772487720140990|1515966223509335414|2268105430162997728|electronics.audio...|  razer|104.14|1515915625452727322|\n",
      "|2020-04-30 21:37:48|2298838016631767159|1515966223509255514|2268105430162997728|electronics.audio...|philips| 32.38|1515915625452815830|\n",
      "|2020-05-01 12:54:33|2299299428894245316|1515966223509089826|2268105406549066706|computers.periphe...|  delux|   6.0|1515915625452840817|\n",
      "|2020-05-01 22:40:42|2299594452823442104|1515966223509122666|2268105430162997728|electronics.audio...|samsung|  8.08|1515915625441990819|\n",
      "|2020-05-02 23:25:58|2300342008322982139|2273948248047616169|2268105406549066706|computers.periphe...|  delux|  5.07|1515915625444068089|\n",
      "|2020-05-04 07:07:11|2301298926818427157|1515966223509351741|2268105419375247608|  stationery.battery|  varta|  0.69|1515915625452091416|\n",
      "|2020-05-06 08:37:23|2302793875086902264|1515966223509090254|2268105421933773102|electronics.audio...|  trust| 19.65|1515915625455538086|\n",
      "|2020-05-06 11:40:59|2302886286089781416|1515966223509089722|2268105441856717530|appliances.kitche...|    ava|  9.24|1515915625455413662|\n",
      "|2020-05-06 18:58:35|2303106530796372754|2273948216456118473|2268105440917193414|appliances.kitche...|polaris| 30.07|1515915625454919094|\n",
      "|2020-05-07 17:06:03|2303774668819006287|1515966223509104779|2268105428166508982|electronics.smart...| huawei|277.75|1515915625455260153|\n",
      "|2020-05-07 17:42:39|2303793094941737810|1515966223509104342|2268105430162997728|electronics.audio...| xiaomi| 25.44|1515915625455337062|\n",
      "|2020-05-08 00:34:21|2304000304833627082|1515966223509117177|2268105464766005446|         kids.skates|ninebot|608.77|1515915625454603469|\n",
      "|2020-05-09 06:14:01|2304896044028134084|1515966223509089293|2360741867017995243|appliances.enviro...|   beko|266.18|1515915625457040117|\n",
      "|2020-05-10 14:59:23|2305885242767966747|2273948308370096764|2268105409048870926|computers.network...|  altel| 57.85|1515915625497428278|\n",
      "|2020-05-10 19:13:40|2306013232357180361|1515966223509088510|2268105428166508982|electronics.smart...| huawei|127.29|1515915625457683949|\n",
      "|2020-05-10 19:37:46|2306025363609748437|1515966223509300544|2268105406691673046|computers.periphe...|   none|  2.75|1515915625455930757|\n",
      "|2020-05-11 08:18:33|2306408274540364573|1515966223509259282|2268105389956399770|appliances.kitche...|indesit|198.13|1515915625457596219|\n",
      "+-------------------+-------------------+-------------------+-------------------+--------------------+-------+------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_cat.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a5c54c-bbf4-49f1-99f4-54b466de1b89",
   "metadata": {},
   "source": [
    "# Count rows where brand is 'none'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b73bfa54-6261-40d8-8d2b-83e49b54a129",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 115:============================>                            (4 + 4) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows where brand is 'none': 13386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "count_none_brand_rows = df_cat.filter(df_cat[\"brand\"] == \"none\").count()\n",
    "\n",
    "print(\"Number of rows where brand is 'none':\", count_none_brand_rows)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c62612-b334-4437-a61f-20c788c3a493",
   "metadata": {},
   "source": [
    "# Drop them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "025e6e01-bb6d-4965-85e6-593d66fc59c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2060046"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_none = df_cat.filter(df_cat[\"brand\"] != \"none\")\n",
    "df_none.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a677418-a004-4501-9861-58612ed15f37",
   "metadata": {},
   "source": [
    "# count the distinct records before the first dot '.' in category_code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "18f66890-cfc0-4489-bddc-dad30c77160d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 127:============================>                            (4 + 4) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of distinct records before the first dot '.': 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import split, col\n",
    "\n",
    "# Extract the category before the first dot '.'\n",
    "df_with_category = df_none.withColumn(\"category_before_dot\", split(col(\"category_code\"), \"\\\\.\")[0])\n",
    "\n",
    "cnt = df_with_category.select(\"category_before_dot\").distinct().count()\n",
    "\n",
    "print(\"Number of distinct records before the first dot '.':\", cnt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4758be-f09c-4823-a246-17577d0b9a48",
   "metadata": {},
   "source": [
    "# Display the distinct category names before the first dot '.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f52a054d-45a4-427f-8f94-2bb028166db4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 133:=======>                                                 (1 + 7) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct names of records before the first dot '.' in the 'category_code' column:\n",
      "medicine\n",
      "computers\n",
      "auto\n",
      "stationery\n",
      "sport\n",
      "other\n",
      "apparel\n",
      "appliances\n",
      "country_yard\n",
      "furniture\n",
      "accessories\n",
      "kids\n",
      "electronics\n",
      "construction\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import split, col\n",
    "\n",
    "# Extract the category before the first dot '.'\n",
    "df_with_category = df_none.withColumn(\"category_before_dot\", split(col(\"category_code\"), \"\\\\.\")[0])\n",
    "\n",
    "# Select and display the distinct names of records before the first dot '.'\n",
    "cat_names = df_with_category.select(\"category_before_dot\").distinct().collect()\n",
    "cat_names = [row[\"category_before_dot\"] for row in cat_names]\n",
    "\n",
    "print(\"Distinct names of records before the first dot '.' in the 'category_code' column:\")\n",
    "for name in cat_names:\n",
    "    print(name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19af980-3032-480b-b5a3-3dff96148199",
   "metadata": {},
   "source": [
    "# Count Records that start with 'string'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b7d487c1-c6ea-4185-9f21-70c5b442ac36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 136:==============>                                          (2 + 6) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "cnt = df_none.filter(col(\"category_code\").startswith('medicine')).count()\n",
    "\n",
    "print(cnt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2e249868-caf6-46e3-b473-0249706652cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 142:============================>                            (4 + 4) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "204806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "cnt = df_none.filter(col(\"category_code\").startswith('computers')).count()\n",
    "\n",
    "print(cnt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4fa7fdab-2442-4dcb-b3d8-7889cd2ed4e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 148:==============>                                          (2 + 6) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3228\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "cnt = df_none.filter(col(\"category_code\").startswith('auto')).count()\n",
    "\n",
    "print(cnt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "391a2570-971f-4282-b639-9678cfb2e305",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 154:=====================>                                   (3 + 5) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38662\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "cnt = df_none.filter(col(\"category_code\").startswith('stationery')).count()\n",
    "\n",
    "print(cnt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "015b5ed0-3352-49fa-ba4f-53215805072d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 160:============================>                            (4 + 4) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "cnt = df_none.filter(col(\"category_code\").startswith('sport')).count()\n",
    "\n",
    "print(cnt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f980728e-9b4c-43cb-8fce-925e7386d918",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 166:==========================================>              (6 + 2) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "547318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "cnt = df_none.filter(col(\"category_code\").startswith('other')).count()\n",
    "\n",
    "print(cnt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ec1d297b-afa6-40e3-8c3e-07e1d6f4e289",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 172:============================>                            (4 + 4) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7599\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "cnt = df_none.filter(col(\"category_code\").startswith('apparel')).count()\n",
    "\n",
    "print(cnt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "23a598fd-585f-4cf7-bc92-ae94d16a8a89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 178:=================================================>       (7 + 1) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "584895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "cnt = df_none.filter(col(\"category_code\").startswith('appliances')).count()\n",
    "\n",
    "print(cnt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8af47274-11ce-4d5b-8841-a3fb864f6c71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 184:==========================================>              (6 + 2) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "cnt = df_none.filter(col(\"category_code\").startswith('country_yard')).count()\n",
    "\n",
    "print(cnt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e388c9a4-e126-41fc-8874-d09a6894be22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 190:===================================>                     (5 + 3) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101190\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "cnt = df_none.filter(col(\"category_code\").startswith('furniture')).count()\n",
    "\n",
    "print(cnt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e7921bbd-6711-4fab-ae0e-5ff2817283f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 196:=====================>                                   (3 + 5) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "cnt = df_none.filter(col(\"category_code\").startswith('accessories')).count()\n",
    "\n",
    "print(cnt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "11f3602a-2bb4-47ea-bcc1-c3c79454ff18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 202:============================>                            (4 + 4) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "cnt = df_none.filter(col(\"category_code\").startswith('kids')).count()\n",
    "\n",
    "print(cnt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1ad9b869-11a1-409a-8189-b0e518ec5a76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 208:=====================>                                   (3 + 5) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "536865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "cnt = df_none.filter(col(\"category_code\").startswith('electronics')).count()\n",
    "\n",
    "print(cnt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "aab33ab3-f944-4b56-a8d5-37b49ffbfd6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 214:==========================================>              (6 + 2) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "cnt = df_none.filter(col(\"category_code\").startswith('construction')).count()\n",
    "\n",
    "print(cnt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1563bf-c706-4696-b4ee-5ac67b313dd1",
   "metadata": {},
   "source": [
    "# Split the category_code column on dot '.' and create new columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c1ba63c7-6525-4a27-9242-b73295aa12b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 220:==========================================>              (6 + 2) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+-------------------+-------------------+--------------------+-------+------+-------------------+-----------+-----------+---------------+\n",
      "|         event_time|           order_id|         product_id|        category_id|       category_code|  brand| price|            user_id|      cat_1|      cat_2|          cat_3|\n",
      "+-------------------+-------------------+-------------------+-------------------+--------------------+-------+------+-------------------+-----------+-----------+---------------+\n",
      "|2020-04-29 20:11:49|2298069964415828136|1515966223509122874|2268105407933187062|computers.periphe...|     hp|152.52|1515915625443027224|  computers|peripherals|        printer|\n",
      "|2020-04-29 23:42:11|2298175846491357353|1515966223509122666|2268105430162997728|electronics.audio...|samsung|  8.08|1515915625445938216|electronics|      audio|      headphone|\n",
      "|2020-04-30 18:01:51|2298729326712980173|1515966223509089265|2360741866917331945|appliances.enviro...|   beko|231.46|1515915625446617606| appliances|environment|air_conditioner|\n",
      "|2020-04-30 19:27:36|2298772487720140990|1515966223509335414|2268105430162997728|electronics.audio...|  razer|104.14|1515915625452727322|electronics|      audio|      headphone|\n",
      "|2020-04-30 21:37:48|2298838016631767159|1515966223509255514|2268105430162997728|electronics.audio...|philips| 32.38|1515915625452815830|electronics|      audio|      headphone|\n",
      "|2020-05-01 12:54:33|2299299428894245316|1515966223509089826|2268105406549066706|computers.periphe...|  delux|   6.0|1515915625452840817|  computers|peripherals|          mouse|\n",
      "|2020-05-01 22:40:42|2299594452823442104|1515966223509122666|2268105430162997728|electronics.audio...|samsung|  8.08|1515915625441990819|electronics|      audio|      headphone|\n",
      "|2020-05-02 23:25:58|2300342008322982139|2273948248047616169|2268105406549066706|computers.periphe...|  delux|  5.07|1515915625444068089|  computers|peripherals|          mouse|\n",
      "|2020-05-04 07:07:11|2301298926818427157|1515966223509351741|2268105419375247608|  stationery.battery|  varta|  0.69|1515915625452091416| stationery|    battery|           NULL|\n",
      "|2020-05-06 08:37:23|2302793875086902264|1515966223509090254|2268105421933773102|electronics.audio...|  trust| 19.65|1515915625455538086|electronics|      audio|     microphone|\n",
      "|2020-05-06 11:40:59|2302886286089781416|1515966223509089722|2268105441856717530|appliances.kitche...|    ava|  9.24|1515915625455413662| appliances|    kitchen|         kettle|\n",
      "|2020-05-06 18:58:35|2303106530796372754|2273948216456118473|2268105440917193414|appliances.kitche...|polaris| 30.07|1515915625454919094| appliances|    kitchen|          mixer|\n",
      "|2020-05-07 17:06:03|2303774668819006287|1515966223509104779|2268105428166508982|electronics.smart...| huawei|277.75|1515915625455260153|electronics| smartphone|           NULL|\n",
      "|2020-05-07 17:42:39|2303793094941737810|1515966223509104342|2268105430162997728|electronics.audio...| xiaomi| 25.44|1515915625455337062|electronics|      audio|      headphone|\n",
      "|2020-05-08 00:34:21|2304000304833627082|1515966223509117177|2268105464766005446|         kids.skates|ninebot|608.77|1515915625454603469|       kids|     skates|           NULL|\n",
      "|2020-05-09 06:14:01|2304896044028134084|1515966223509089293|2360741867017995243|appliances.enviro...|   beko|266.18|1515915625457040117| appliances|environment|air_conditioner|\n",
      "|2020-05-10 14:59:23|2305885242767966747|2273948308370096764|2268105409048870926|computers.network...|  altel| 57.85|1515915625497428278|  computers|    network|         router|\n",
      "|2020-05-10 19:13:40|2306013232357180361|1515966223509088510|2268105428166508982|electronics.smart...| huawei|127.29|1515915625457683949|electronics| smartphone|           NULL|\n",
      "|2020-05-11 08:18:33|2306408274540364573|1515966223509259282|2268105389956399770|appliances.kitche...|indesit|198.13|1515915625457596219| appliances|    kitchen|         washer|\n",
      "|2020-05-11 10:41:08|2306480038981140584|1515966223509088613|2268105430162997728|electronics.audio...|  apple|203.68|1515915625457954091|electronics|      audio|      headphone|\n",
      "+-------------------+-------------------+-------------------+-------------------+--------------------+-------+------+-------------------+-----------+-----------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import split\n",
    "\n",
    "split_col = split(df_none['category_code'], '\\.')\n",
    "\n",
    "df_dot = df_none.withColumn('cat_1', split_col.getItem(0))\n",
    "df_dot = df_dot.withColumn('cat_2', split_col.getItem(1))\n",
    "df_dot = df_dot.withColumn('cat_3', split_col.getItem(2))\n",
    "\n",
    "df_dot.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d340f100-3e2f-4542-9444-f43242e93285",
   "metadata": {},
   "source": [
    "# Drop category_code column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "dd0e67c1-fb24-43c2-acb1-0bec2efc9f45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 223:===================================>                     (5 + 3) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+-------------------+-------------------+-------+------+-------------------+-----------+-----------+---------------+\n",
      "|         event_time|           order_id|         product_id|        category_id|  brand| price|            user_id|      cat_1|      cat_2|          cat_3|\n",
      "+-------------------+-------------------+-------------------+-------------------+-------+------+-------------------+-----------+-----------+---------------+\n",
      "|2020-04-29 20:11:49|2298069964415828136|1515966223509122874|2268105407933187062|     hp|152.52|1515915625443027224|  computers|peripherals|        printer|\n",
      "|2020-04-29 23:42:11|2298175846491357353|1515966223509122666|2268105430162997728|samsung|  8.08|1515915625445938216|electronics|      audio|      headphone|\n",
      "|2020-04-30 18:01:51|2298729326712980173|1515966223509089265|2360741866917331945|   beko|231.46|1515915625446617606| appliances|environment|air_conditioner|\n",
      "|2020-04-30 19:27:36|2298772487720140990|1515966223509335414|2268105430162997728|  razer|104.14|1515915625452727322|electronics|      audio|      headphone|\n",
      "|2020-04-30 21:37:48|2298838016631767159|1515966223509255514|2268105430162997728|philips| 32.38|1515915625452815830|electronics|      audio|      headphone|\n",
      "|2020-05-01 12:54:33|2299299428894245316|1515966223509089826|2268105406549066706|  delux|   6.0|1515915625452840817|  computers|peripherals|          mouse|\n",
      "|2020-05-01 22:40:42|2299594452823442104|1515966223509122666|2268105430162997728|samsung|  8.08|1515915625441990819|electronics|      audio|      headphone|\n",
      "|2020-05-02 23:25:58|2300342008322982139|2273948248047616169|2268105406549066706|  delux|  5.07|1515915625444068089|  computers|peripherals|          mouse|\n",
      "|2020-05-04 07:07:11|2301298926818427157|1515966223509351741|2268105419375247608|  varta|  0.69|1515915625452091416| stationery|    battery|           NULL|\n",
      "|2020-05-06 08:37:23|2302793875086902264|1515966223509090254|2268105421933773102|  trust| 19.65|1515915625455538086|electronics|      audio|     microphone|\n",
      "|2020-05-06 11:40:59|2302886286089781416|1515966223509089722|2268105441856717530|    ava|  9.24|1515915625455413662| appliances|    kitchen|         kettle|\n",
      "|2020-05-06 18:58:35|2303106530796372754|2273948216456118473|2268105440917193414|polaris| 30.07|1515915625454919094| appliances|    kitchen|          mixer|\n",
      "|2020-05-07 17:06:03|2303774668819006287|1515966223509104779|2268105428166508982| huawei|277.75|1515915625455260153|electronics| smartphone|           NULL|\n",
      "|2020-05-07 17:42:39|2303793094941737810|1515966223509104342|2268105430162997728| xiaomi| 25.44|1515915625455337062|electronics|      audio|      headphone|\n",
      "|2020-05-08 00:34:21|2304000304833627082|1515966223509117177|2268105464766005446|ninebot|608.77|1515915625454603469|       kids|     skates|           NULL|\n",
      "|2020-05-09 06:14:01|2304896044028134084|1515966223509089293|2360741867017995243|   beko|266.18|1515915625457040117| appliances|environment|air_conditioner|\n",
      "|2020-05-10 14:59:23|2305885242767966747|2273948308370096764|2268105409048870926|  altel| 57.85|1515915625497428278|  computers|    network|         router|\n",
      "|2020-05-10 19:13:40|2306013232357180361|1515966223509088510|2268105428166508982| huawei|127.29|1515915625457683949|electronics| smartphone|           NULL|\n",
      "|2020-05-11 08:18:33|2306408274540364573|1515966223509259282|2268105389956399770|indesit|198.13|1515915625457596219| appliances|    kitchen|         washer|\n",
      "|2020-05-11 10:41:08|2306480038981140584|1515966223509088613|2268105430162997728|  apple|203.68|1515915625457954091|electronics|      audio|      headphone|\n",
      "+-------------------+-------------------+-------------------+-------------------+-------+------+-------------------+-----------+-----------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_nocc = df_dot.drop(\"category_code\")\n",
    "df_nocc.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9308a997-81ae-4b08-93ca-ef56e14f5469",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 228:>                                                        (0 + 8) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+----------+-----------+-----+-----+-------+-----+------+-------+\n",
      "|event_time|order_id|product_id|category_id|brand|price|user_id|cat_1| cat_2|  cat_3|\n",
      "+----------+--------+----------+-----------+-----+-----+-------+-----+------+-------+\n",
      "|         0|       0|         0|          0|    0|    0|1527981|    0|547318|1139814|\n",
      "+----------+--------+----------+-----------+-----+-----+-------+-----+------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "null_cnt = [F.sum(F.col(column).isNull().cast(\"integer\")).alias(column) for column in df_nocc.columns]\n",
    "\n",
    "null_cnt_df = df_nocc.agg(*null_cnt)\n",
    "\n",
    "null_cnt_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea3e7fe-eeb4-4c96-be4c-88ee39162ba6",
   "metadata": {},
   "source": [
    "# Assign \"other\" to null values in cat_2 and cat_3 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "30cf3417-76f3-4dec-abe1-3a05a87e0b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nocc = df_nocc.fillna(\"other\", subset=[\"cat_2\", \"cat_3\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7460bb5e-b1bb-463a-b9d0-dea1c4493a97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 234:>                                                        (0 + 8) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+----------+-----------+-----+-----+-------+-----+-----+-----+\n",
      "|event_time|order_id|product_id|category_id|brand|price|user_id|cat_1|cat_2|cat_3|\n",
      "+----------+--------+----------+-----------+-----+-----+-------+-----+-----+-----+\n",
      "|         0|       0|         0|          0|    0|    0|1527981|    0|    0|    0|\n",
      "+----------+--------+----------+-----------+-----+-----+-------+-----+-----+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "null_cnt = [F.sum(F.col(column).isNull().cast(\"integer\")).alias(column) for column in df_nocc.columns]\n",
    "\n",
    "null_cnt_df = df_nocc.agg(*null_cnt)\n",
    "\n",
    "null_cnt_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12388a6-f849-4a37-8030-55ee3765dd80",
   "metadata": {},
   "source": [
    "# Distinct categories in cat 1,2,3 column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c34572ec-ada7-48ae-8a7e-73461a82c737",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 238:============================>                            (4 + 4) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "cnt = df_nocc.select(\"cat_1\").distinct().count()\n",
    "\n",
    "print(cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7c352706-4858-47ec-ac6f-d0d3c3ebec18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 244:=======>                                                 (1 + 7) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "cnt = df_nocc.select(\"cat_2\").distinct().count()\n",
    "\n",
    "print(cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1e683805-a30e-44c5-858e-4f12f9f03cb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 250:=======>                                                 (1 + 7) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "cnt = df_nocc.select(\"cat_3\").distinct().count()\n",
    "\n",
    "print(cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1592476b-4fc9-4543-a602-975c3a5508f3",
   "metadata": {},
   "source": [
    "# Count rows where price is 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "593aecb0-337a-44c8-8a3b-65323404fad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 256:=====================>                                   (3 + 5) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records where price is 0: 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "cnt = df_nocc.filter(df_nocc[\"price\"] == 0).count()\n",
    "\n",
    "print(\"Number of records where price is 0:\", cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a01e2d-62d2-4d81-93ff-9e2ea7ada654",
   "metadata": {},
   "source": [
    "# Download clean data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "5f64a82b-d8b1-47cd-95fc-6e23c8cdfc2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# output_path = \"noCC\"\n",
    "# df_nocc.coalesce(1).write.csv(output_path, header=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef76697f-52b3-4ceb-918c-fa2e672c223b",
   "metadata": {},
   "source": [
    "# Load file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "382a43ed-4b4a-4e70-b1f9-fa691c7b2e2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "data_path = \"nocc.csv\"\n",
    "df_nocc = spark.read.csv(data_path, header=True, inferSchema=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e3f245-919b-453f-bf7f-5704d8d789a0",
   "metadata": {},
   "source": [
    "# Replace zeros with the average price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f7729172-7dda-4285-a28b-0b6689c05276",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg, when\n",
    "\n",
    "# Calculate the average price (excluding zeros)\n",
    "avg_price = df_nocc.filter(df_nocc[\"price\"] != 0).agg(avg(\"price\")).collect()[0][0]\n",
    "\n",
    "# Replace\n",
    "df_avgp = df_nocc.withColumn(\"price\", when(df_nocc[\"price\"] == 0, avg_price).otherwise(df_nocc[\"price\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5226c28-8fd9-42d4-917b-720205a78356",
   "metadata": {},
   "source": [
    "# check zero's count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c65b9582-4ea1-42ce-9a38-e887020ff114",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 267:>                                                        (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records where price is 0: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "cnt = df_avgp.filter(df_avgp[\"price\"] == 0).count()\n",
    "\n",
    "print(\"Number of records where price is 0:\", cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27db925-7aca-4249-a3af-1b028a201464",
   "metadata": {},
   "source": [
    "# Calculate the total amount spent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2f6ccd51-628d-498b-b32e-07bdd591ea0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total amount spent: 330064703.276678\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, sum\n",
    "\n",
    "total_amount = df_avgp.select(sum(col(\"price\"))).collect()[0][0]\n",
    "\n",
    "print(\"Total amount spent:\", total_amount)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ec880b-6dab-4698-a7dd-3ed47f2f84c8",
   "metadata": {},
   "source": [
    "# Max spent by user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7fbf50b1-8242-4ae5-b0c9-b6c9f7bbf1c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 273:>                                                        (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max spent: 50925.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import max\n",
    "\n",
    "max = df_avgp.agg(max(\"price\")).collect()[0][0]\n",
    "\n",
    "print(\"Max spent:\", max)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d836f562-eef3-4ed4-9f00-29539242e0e0",
   "metadata": {},
   "source": [
    "# Min and Mean spent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5b90fb0d-236f-43bd-a301-d59a36fa7e2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 276:=======>                                                 (1 + 7) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min spent: 0.02\n",
      "Mean spent: 160.2220063419351\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import min, mean\n",
    "\n",
    "statistics = df_avgp.agg(min(\"price\"), mean(\"price\")).collect()[0]\n",
    "\n",
    "min = statistics[0]\n",
    "mean = statistics[1]\n",
    "\n",
    "print(\"Min spent:\", min)\n",
    "print(\"Mean spent:\", mean)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f347a5a-4476-4c3e-82a1-19f24431268c",
   "metadata": {},
   "source": [
    "# Download and load file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "a73692bb-5dd8-426e-a16d-6576adeb09c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# output_path = \"avgPrice\"\n",
    "# df_avgp.coalesce(1).write.csv(output_path, header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "033c15ad-fc4b-407d-abee-c9b5da209b10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "data_path = \"avgp.csv\"\n",
    "df_avgp = spark.read.csv(data_path, header=True, inferSchema=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "83a7df19-51f8-40a9-9837-0b007571f1f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2060046"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_avgp.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d1b773-0ee3-4bfe-8ae4-15f302a21cd5",
   "metadata": {},
   "source": [
    "# Count occurrences of \"other\" in the cat_1 column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5889b17d-d301-4c79-8665-0ce3d073562c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 284:=======>                                                 (1 + 7) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of 'other' in cat_1 column: 547318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "cnt = df_avgp.filter(col(\"cat_1\") == \"other\").count()\n",
    "\n",
    "print(\"Count of 'other' in cat_1 column:\", cnt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3589954-1f15-4bea-a91f-a87168096ae2",
   "metadata": {},
   "source": [
    "# Assign \"miscellaneous\" where the value is \"other\" in the cat1 column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e78dbafa-51b3-4f7e-bf0c-f4cfb91cafc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when\n",
    "df_avgp = df_avgp.withColumn(\"cat_1\", when(col(\"cat_1\") == \"other\", \"miscellaneous\").otherwise(col(\"cat_1\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "840b0e47-75fb-4498-88fe-a51c9088bf65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 287:>                                                        (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of 'miscellaneous' in cat_1 column: 547318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "cnt = df_avgp.filter(col(\"cat_1\") == \"miscellaneous\").count()\n",
    "\n",
    "print(\"Count of 'miscellaneous' in cat_1 column:\", cnt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630258c9-756c-402a-b629-565afbcb5818",
   "metadata": {},
   "source": [
    "# Do same for cat_2 and cat_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1ad811fe-c72a-40e1-9840-c3f5c80e46f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 290:>                                                        (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of 'other' in cat_2 column: 547318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "cnt = df_avgp.filter(col(\"cat_2\") == \"other\").count()\n",
    "\n",
    "print(\"Count of 'other' in cat_2 column:\", cnt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "08f12671-0ef2-49b0-82a1-062b8a051ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when\n",
    "df_avgp = df_avgp.withColumn(\"cat_2\", when(col(\"cat_2\") == \"other\", \"miscellaneous\").otherwise(col(\"cat_2\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "4d910b5a-0d7e-4907-9c96-9e946d3cd833",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 293:>                                                        (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of 'miscellaneous' in cat_2 column: 547318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "cnt = df_avgp.filter(col(\"cat_2\") == \"miscellaneous\").count()\n",
    "\n",
    "print(\"Count of 'miscellaneous' in cat_2 column:\", cnt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ada46f-ae64-4a65-bbe2-45028d157c3e",
   "metadata": {},
   "source": [
    "# cat_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "07a38dab-e27c-4e16-8283-a77e85774c3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of 'other' in cat_3 column: 1139814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "cnt = df_avgp.filter(col(\"cat_3\") == \"other\").count()\n",
    "\n",
    "print(\"Count of 'other' in cat_3 column:\", cnt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "be1437ae-6d65-416c-b15d-9c0f1ad61d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when\n",
    "df_avgp = df_avgp.withColumn(\"cat_3\", when(col(\"cat_3\") == \"other\", \"miscellaneous\").otherwise(col(\"cat_3\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "fe3eadcb-a504-4314-ab17-427e8ce48014",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 302:=======>                                                 (1 + 7) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of 'miscellaneous' in cat_3 column: 1139814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "cnt = df_avgp.filter(col(\"cat_3\") == \"miscellaneous\").count()\n",
    "\n",
    "print(\"Count of 'miscellaneous' in cat_3 column:\", cnt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed58e2af-c7bf-485e-9302-120a9f8803b9",
   "metadata": {},
   "source": [
    "# Count distinct user IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "91000dcf-4b1d-43ed-a00c-01c9a38cd2cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "228415"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_avgp.select(\"user_id\").distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa646d9-8aab-4a9c-99ef-e5c28218013c",
   "metadata": {},
   "source": [
    "# Check null user ID count in dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "fde41ccd-d917-4182-82cd-2d010368daf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 311:===================================>                     (5 + 3) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+----------+-----------+-----+-----+-------+-----+-----+-----+\n",
      "|event_time|order_id|product_id|category_id|brand|price|user_id|cat_1|cat_2|cat_3|\n",
      "+----------+--------+----------+-----------+-----+-----+-------+-----+-----+-----+\n",
      "|         0|       0|         0|          0|    0|    0|1527981|    0|    0|    0|\n",
      "+----------+--------+----------+-----------+-----+-----+-------+-----+-----+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "null_cnt = [F.sum(F.col(column).isNull().cast(\"integer\")).alias(column) for column in df_avgp.columns]\n",
    "\n",
    "null_cnt_df = df_avgp.agg(*null_cnt)\n",
    "\n",
    "null_cnt_df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266b205f-08f8-47ed-b332-c476c05dfa45",
   "metadata": {},
   "source": [
    "# Download file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a602aa53-58ac-4a2b-9448-5aa473a53023",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_path = \"Misc\"\n",
    "# df_avgp.coalesce(1).write.csv(output_path, header=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04581548-38f4-40df-a199-16ef8dd9e58b",
   "metadata": {},
   "source": [
    "# Load file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "a7e52c81-b4bd-4210-8fe4-7aa94e107cd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "data_path = \"misc.csv\"\n",
    "df_misc = spark.read.csv(data_path, header=True, inferSchema=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "b96b622b-2a2f-455f-939a-b7311f136f03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2060046"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_misc.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "dec15d89-c89d-46ae-9d5f-fe6e542a8a3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "228415"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_misc.select(\"user_id\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "dbba0b7d-06ae-40e5-83c8-1c04bc96dafc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "228414"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import collect_set\n",
    "\n",
    "# Step 1: Group the DataFrame by user_id and collect the set of user IDs\n",
    "uniquser = df_misc.groupBy().agg(collect_set(\"user_id\").alias(\"unique_user_ids\")).first()[\"unique_user_ids\"]\n",
    "\n",
    "len(uniquser)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ddebd2d-30a6-417b-a6c9-a9743dd73ece",
   "metadata": {},
   "source": [
    "# Select the cat_1 column and extract distinct values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "9a3c9be3-a0fd-4c51-85a3-90fc9eb2f958",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distinct_categories = df_misc.select(\"cat_1\").distinct()\n",
    "\n",
    "# Convert the DataFrame of distinct categories into a list\n",
    "uniqcat = [row.cat_1 for row in distinct_categories.collect()]\n",
    "\n",
    "len(uniqcat)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d756fd5a-37bc-4f46-aad6-8b26e2a498e5",
   "metadata": {},
   "source": [
    "# Count no. of unique users for each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "863635e6-b6c7-494e-84c7-206c69439213",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/23 14:46:53 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "[Stage 356:>                                                        (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category: medicine\n",
      "No. of Unique User IDs: 968\n",
      "\n",
      "Category: computers\n",
      "No. of Unique User IDs: 41920\n",
      "\n",
      "Category: auto\n",
      "No. of Unique User IDs: 1081\n",
      "\n",
      "Category: stationery\n",
      "No. of Unique User IDs: 6084\n",
      "\n",
      "Category: sport\n",
      "No. of Unique User IDs: 594\n",
      "\n",
      "Category: apparel\n",
      "No. of Unique User IDs: 1689\n",
      "\n",
      "Category: appliances\n",
      "No. of Unique User IDs: 85986\n",
      "\n",
      "Category: country_yard\n",
      "No. of Unique User IDs: 158\n",
      "\n",
      "Category: furniture\n",
      "No. of Unique User IDs: 14221\n",
      "\n",
      "Category: accessories\n",
      "No. of Unique User IDs: 2581\n",
      "\n",
      "Category: kids\n",
      "No. of Unique User IDs: 1645\n",
      "\n",
      "Category: electronics\n",
      "No. of Unique User IDs: 97271\n",
      "\n",
      "Category: construction\n",
      "No. of Unique User IDs: 3107\n",
      "\n",
      "Category: miscellaneous\n",
      "No. of Unique User IDs: 67841\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import countDistinct\n",
    "\n",
    "spark = SparkSession.builder.appName(\"DistinctUsersByCategory\").getOrCreate()\n",
    "\n",
    "\n",
    "# Group the DataFrame by cat_1 and count the distinct user IDs\n",
    "distinct_users_count_by_category = df_misc.groupBy(\"cat_1\").agg(countDistinct(\"user_id\").alias(\"unique_user_count\"))\n",
    "\n",
    "# Collect the results as a list of tuples containing category and unique user count\n",
    "categories_with_user_counts = distinct_users_count_by_category.rdd.map(lambda x: (x.cat_1, x.unique_user_count)).collect()\n",
    "\n",
    "# Print the category and corresponding count of unique user IDs\n",
    "for category, user_count in categories_with_user_counts:\n",
    "    print(\"Category:\", category)\n",
    "    print(\"No. of Unique User IDs:\", user_count)\n",
    "    print()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39425a4b-f557-4412-bf88-eda7b4b5bbd5",
   "metadata": {},
   "source": [
    "# Assign a random user ID based on the category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "2ad25b2a-0bac-4560-9a97-00194b578c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from random import choice\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "spark = SparkSession.builder.appName(\"AssignNewUserID\").getOrCreate()\n",
    "\n",
    "\n",
    "# Convert the list of tuples to a dictionary for efficient lookup\n",
    "category_user_dict = dict(categories_with_user_counts)\n",
    "\n",
    "# Define a UDF to assign a random user ID based on the category\n",
    "@udf(StringType())\n",
    "def assign_new_user_id(category):\n",
    "    if category in category_user_dict:\n",
    "        return str(choice(category_user_dict[category]))\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Add the new column new_user_id and assign values using the UDF\n",
    "df_misc = df_misc.withColumn(\"new_user_id\", assign_new_user_id(df_misc[\"cat_1\"]))\n",
    "\n",
    "df_misc.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "424420c5-5175-4cd5-85b2-84d3182de5f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 337:==========================================>              (6 + 2) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+----------+-----------+-----+-----+-------+-----+-----+-----+\n",
      "|event_time|order_id|product_id|category_id|brand|price|user_id|cat_1|cat_2|cat_3|\n",
      "+----------+--------+----------+-----------+-----+-----+-------+-----+-----+-----+\n",
      "|         0|       0|         0|          0|    0|    0|1527981|    0|    0|    0|\n",
      "+----------+--------+----------+-----------+-----+-----+-------+-----+-----+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "null_cnt = [F.sum(F.col(column).isNull().cast(\"integer\")).alias(column) for column in df_misc.columns]\n",
    "\n",
    "null_cnt_df = df_misc.agg(*null_cnt)\n",
    "\n",
    "null_cnt_df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b9eae1-c86b-40d6-9cc0-b83d244bccd6",
   "metadata": {},
   "source": [
    "# Check count no. of unique users for each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5eff5498-9672-4938-94d5-e46d7d6f2ece",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 95:=======>                                                  (1 + 7) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category: medicine\n",
      "No. of Unique User IDs: 944\n",
      "\n",
      "Category: computers\n",
      "No. of Unique User IDs: 40550\n",
      "\n",
      "Category: auto\n",
      "No. of Unique User IDs: 1023\n",
      "\n",
      "Category: stationery\n",
      "No. of Unique User IDs: 6066\n",
      "\n",
      "Category: sport\n",
      "No. of Unique User IDs: 572\n",
      "\n",
      "Category: apparel\n",
      "No. of Unique User IDs: 1672\n",
      "\n",
      "Category: appliances\n",
      "No. of Unique User IDs: 80818\n",
      "\n",
      "Category: country_yard\n",
      "No. of Unique User IDs: 135\n",
      "\n",
      "Category: furniture\n",
      "No. of Unique User IDs: 14184\n",
      "\n",
      "Category: accessories\n",
      "No. of Unique User IDs: 2565\n",
      "\n",
      "Category: kids\n",
      "No. of Unique User IDs: 1561\n",
      "\n",
      "Category: electronics\n",
      "No. of Unique User IDs: 89913\n",
      "\n",
      "Category: construction\n",
      "No. of Unique User IDs: 3035\n",
      "\n",
      "Category: miscellaneous\n",
      "No. of Unique User IDs: 64115\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import countDistinct\n",
    "\n",
    "spark = SparkSession.builder.appName(\"DistinctUsersByCategory\").getOrCreate()\n",
    "\n",
    "# Group the DataFrame by cat_1 and count the distinct user IDs\n",
    "distinct_users_count_by_category = df_misc.groupBy(\"cat_1\").agg(countDistinct(\"new_user_id\").alias(\"unique_user_count\"))\n",
    "\n",
    "# Collect the results as a list of tuples containing category and unique user count\n",
    "categories_with_user_counts = distinct_users_count_by_category.rdd.map(lambda x: (x.cat_1, x.unique_user_count)).collect()\n",
    "\n",
    "# Print the category and corresponding count of unique user IDs\n",
    "for category, user_count in categories_with_user_counts:\n",
    "    print(\"Category:\", category)\n",
    "    print(\"No. of Unique User IDs:\", user_count)\n",
    "    print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "7d98f47f-eceb-4bc1-9035-758a8700e711",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "228415"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_misc.select(\"user_id\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "86d6d128-f325-4c71-a3e6-742893aa35dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_misc.select(\"new_user_id\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "75146da1-3d75-4e83-a2e7-d5a739721bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filnul= df_misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7bd4e361-d2cf-4ec7-be77-1dd54a00350b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# output_path = \"FillNullUser\"\n",
    "# df_filnul.coalesce(1).write.csv(output_path, header=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef3b8d2-aab4-4ecd-bb52-0c3adfd186dc",
   "metadata": {},
   "source": [
    "# Drop user_id "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c22b63ff-2f1e-4a0c-a0da-4333dd11f743",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nonull = df_filnul.drop(\"user_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6481af2c-0b70-409d-af3a-ac4173f0c30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nonull = df_nonull.withColumnRenamed(\"new_user_id\", \"user_id\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7822a465-4703-4fc8-b3b0-dbba10494db8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+-------------------+-------------------+-------+------+-----------+-----------+---------------+-------------------+\n",
      "|         event_time|           order_id|         product_id|        category_id|  brand| price|      cat_1|      cat_2|          cat_3|            user_id|\n",
      "+-------------------+-------------------+-------------------+-------------------+-------+------+-----------+-----------+---------------+-------------------+\n",
      "|2020-04-29 20:11:49|2298069964415828136|1515966223509122874|2268105407933187062|     hp|152.52|  computers|peripherals|        printer|1515915625509647001|\n",
      "|2020-04-29 23:42:11|2298175846491357353|1515966223509122666|2268105430162997728|samsung|  8.08|electronics|      audio|      headphone|1515915625511889093|\n",
      "|2020-04-30 18:01:51|2298729326712980173|1515966223509089265|2360741866917331945|   beko|231.46| appliances|environment|air_conditioner|1515915625510823948|\n",
      "|2020-04-30 19:27:36|2298772487720140990|1515966223509335414|2268105430162997728|  razer|104.14|electronics|      audio|      headphone|1515915625512018088|\n",
      "|2020-04-30 21:37:48|2298838016631767159|1515966223509255514|2268105430162997728|philips| 32.38|electronics|      audio|      headphone|1515915625499309469|\n",
      "|2020-05-01 12:54:33|2299299428894245316|1515966223509089826|2268105406549066706|  delux|   6.0|  computers|peripherals|          mouse|1515915625514536341|\n",
      "|2020-05-01 22:40:42|2299594452823442104|1515966223509122666|2268105430162997728|samsung|  8.08|electronics|      audio|      headphone|1515915625484635108|\n",
      "|2020-05-02 23:25:58|2300342008322982139|2273948248047616169|2268105406549066706|  delux|  5.07|  computers|peripherals|          mouse|1515915625484676605|\n",
      "|2020-05-04 07:07:11|2301298926818427157|1515966223509351741|2268105419375247608|  varta|  0.69| stationery|    battery|  miscellaneous|1515915625512202402|\n",
      "|2020-05-06 08:37:23|2302793875086902264|1515966223509090254|2268105421933773102|  trust| 19.65|electronics|      audio|     microphone|1515915625484676302|\n",
      "|2020-05-06 11:40:59|2302886286089781416|1515966223509089722|2268105441856717530|    ava|  9.24| appliances|    kitchen|         kettle|1515915625512660612|\n",
      "|2020-05-06 18:58:35|2303106530796372754|2273948216456118473|2268105440917193414|polaris| 30.07| appliances|    kitchen|          mixer|1515915625492249288|\n",
      "|2020-05-07 17:06:03|2303774668819006287|1515966223509104779|2268105428166508982| huawei|277.75|electronics| smartphone|  miscellaneous|1515915625475938346|\n",
      "|2020-05-07 17:42:39|2303793094941737810|1515966223509104342|2268105430162997728| xiaomi| 25.44|electronics|      audio|      headphone|1515915625510239770|\n",
      "|2020-05-08 00:34:21|2304000304833627082|1515966223509117177|2268105464766005446|ninebot|608.77|       kids|     skates|  miscellaneous|1515915625511698424|\n",
      "|2020-05-09 06:14:01|2304896044028134084|1515966223509089293|2360741867017995243|   beko|266.18| appliances|environment|air_conditioner|1515915625448095694|\n",
      "|2020-05-10 14:59:23|2305885242767966747|2273948308370096764|2268105409048870926|  altel| 57.85|  computers|    network|         router|1515915625513605277|\n",
      "|2020-05-10 19:13:40|2306013232357180361|1515966223509088510|2268105428166508982| huawei|127.29|electronics| smartphone|  miscellaneous|1515915625511195241|\n",
      "|2020-05-11 08:18:33|2306408274540364573|1515966223509259282|2268105389956399770|indesit|198.13| appliances|    kitchen|         washer|1515915625512118829|\n",
      "|2020-05-11 10:41:08|2306480038981140584|1515966223509088613|2268105430162997728|  apple|203.68|electronics|      audio|      headphone|1515915625514504268|\n",
      "+-------------------+-------------------+-------------------+-------------------+-------+------+-----------+-----------+---------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_nonull.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c9f165a9-580b-4294-89fd-a0e06ae8a915",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 115:=================================================>       (7 + 1) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+----------+-----------+-----+-----+-----+-----+-----+-------+\n",
      "|event_time|order_id|product_id|category_id|brand|price|cat_1|cat_2|cat_3|user_id|\n",
      "+----------+--------+----------+-----------+-----+-----+-----+-----+-----+-------+\n",
      "|         0|       0|         0|          0|    0|    0|    0|    0|    0|      0|\n",
      "+----------+--------+----------+-----------+-----+-----+-----+-----+-----+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "null_cnt = [F.sum(F.col(column).isNull().cast(\"integer\")).alias(column) for column in df_nonull.columns]\n",
    "\n",
    "null_cnt_df = df_nonull.agg(*null_cnt)\n",
    "\n",
    "null_cnt_df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b046abe4-4e40-47bc-a0f5-05057a00fc00",
   "metadata": {},
   "source": [
    "# Download No null Data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4f6066c0-bd2f-40e7-82e6-a4a588da57cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# output_path = \"NoNullData\"\n",
    "# df_nonull.coalesce(1).write.csv(output_path, header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7189203f-829a-4619-8bf8-143d2362397d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df_nonull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "05263bd6-a464-4540-8e43-7e822ef5c291",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "907"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(\"brand\").distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07cdefe3-387e-434b-a49d-7eb6ce3fa217",
   "metadata": {},
   "source": [
    "# Total price in clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "47b65554-7439-4753-a95a-0fa99447c38f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 191:============================>                            (4 + 4) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Price: 330064703.276678\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "total_price = df.agg(F.sum(\"price\")).collect()[0][0]\n",
    "\n",
    "print(\"Total Price:\", total_price)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
